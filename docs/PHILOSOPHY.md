# A Vibecoder's Guide to Image Analysis: Philosophy

## What is vibecoding?

Vibecoding is a term coined in 2025 to describe a new mode of software development in which a person describes what they want in natural language and an AI assistant writes the code. The vibecoder doesn't read every line of output. They don't debug stack traces. They describe the *vibe* — the intent, the feel, the desired outcome — and iterate until the result matches their expectations.

This sounds reckless. In many contexts, it is. But in one specific domain, vibecoding is not just acceptable — it is transformative: **scientific image analysis by biologists who understand their images but cannot write code.**

## The gap

Modern cell biology generates enormous volumes of fluorescence microscopy data. Extracting quantitative measurements from these images — segmenting cells, detecting puncta, measuring colocalization, computing spatial relationships — requires computational tools. The tools exist. CellProfiler, Fiji/ImageJ, ilastik, Cellpose, StarDist, and dozens of others provide powerful, validated algorithms for every step of a typical analysis pipeline.

The problem is not a lack of tools. The problem is **assembly**. A biologist studying stress granules needs to:

1. Segment cells from a multi-channel fluorescence image
2. Optionally segment nuclei
3. Detect punctate structures in one or more channels
4. Quantify per-cell metrics (number of puncta, fraction of signal condensed, etc.)
5. Handle biological replicates correctly in the statistics
6. Generate publication-quality plots

Each of these steps has well-validated solutions. But connecting them into a coherent pipeline — loading the right file format, passing the output of step 1 into step 2, handling edge cases, formatting the output for statistical analysis — requires programming. Not advanced programming. Not novel algorithms. Just the mundane, tedious, error-prone plumbing of reading files, reshaping arrays, and writing CSVs.

This plumbing is exactly what AI coding assistants are good at.

## The vibecoder's advantage

A biologist looking at a fluorescence image knows things that a computer scientist does not:

- **Which structures are real.** That bright spot in the DAPI channel is a nucleus, not an artifact. Those puncta in the GFP channel are stress granules, not autofluorescence.
- **What constitutes a good segmentation.** The algorithm split a mother-daughter yeast pair into two cells? Good. It merged two adjacent cells that are clearly distinct? Bad.
- **What the biologically meaningful measurements are.** The fraction of signal in puncta matters. The mean fluorescence intensity might not.
- **What constitutes a reasonable result.** Arsenite-treated cells should have more stress granules than controls. If the pipeline says otherwise, something is wrong.

These judgments require years of training in cell biology. They are precisely what cannot be automated. But they are also precisely what is needed to *supervise* an automated pipeline — to evaluate whether the output is correct and iterate until it is.

The vibecoder workflow inverts the traditional relationship between biologist and code. Instead of the biologist learning to program in order to use computational tools, the biologist uses their domain expertise to evaluate and refine outputs generated by an AI assistant. The biologist is not the programmer. The biologist is the **reviewer**.

## Design principles

This guide and the accompanying `cellquant.py` pipeline are built on several principles:

### One script, not a package

`cellquant.py` is a single Python file. Not a package. Not a library. Not something you `pip install`. You download one file, and you run it.

This is a deliberate choice. Packages have dependencies, version conflicts, import errors, and installation procedures that are each individually minor but collectively lethal to a non-programmer. A single file can be read, moved, emailed, and understood as a unit. When something goes wrong, there is exactly one place to look.

### The command line is the interface

All customization happens through command-line arguments. The user never edits Python code. Changing the analysis from mammalian cells to yeast means changing `--cell-type mammalian` to `--cell-type yeast`. Adding colocalization analysis means adding `--colocalization`. The interface is plain English with dashes.

This matters because command-line arguments are *legible to an AI assistant*. A biologist can tell ChatGPT or Claude: "I have three-channel yeast images with Sis1, Nsr1, and Tif6, and I want to measure colocalization and nucleolar proximity." The AI can translate this into the correct command-line invocation without the biologist understanding what any of the flags do programmatically.

### Cell-type presets encode domain knowledge

The hardest parameters to set are the ones that depend on biology: How big are cells in pixels? Should you downsample before segmentation? What area range is reasonable? These parameters differ fundamentally between mammalian cells, yeast, and bacteria.

Cell-type presets (`--cell-type mammalian`, `--cell-type yeast`, `--cell-type bacteria`) encode sensible defaults for each organism. The user picks the preset that matches their cells, and the pipeline handles the rest. Individual parameters can be overridden when needed, but the defaults should work for the majority of cases.

### Validate visually, not programmatically

Every image processed by the pipeline produces a QC overlay: the original fluorescence image with segmentation boundaries, puncta markers, and nucleolar outlines drawn on top. The biologist looks at these overlays and evaluates whether the segmentation is correct. This is the validation step. It requires exactly the domain expertise that the biologist already has.

If the overlays look wrong, the biologist adjusts parameters (with AI assistance) and re-runs. This iterative, visual refinement loop is the core workflow. It does not require understanding the algorithm. It requires understanding the biology.

### Statistics should be honest

Biological image analysis has a statistics problem. It is tempting to treat each cell as an independent observation, run a t-test on thousands of cells, and report a vanishingly small p-value. This is wrong. Cells from the same image are not independent. Images from the same biological replicate are not independent.

The pipeline implements superplot-style analysis: per-cell measurements are visualized and summarized, but statistical tests operate on replicate-level summaries (medians). This reduces statistical power — sometimes dramatically — but produces honest results. An experiment with two biological replicates per condition should not produce p = 10⁻²⁰, no matter how many cells it contains.

The guide demonstrates this explicitly. The mammalian stress granule data show clear visual effects with p-values that hover around 0.05–0.15, because three biological replicates are simply not enough to power a replicate-level test for modest effect sizes. This is not a failure of the pipeline. It is an honest reflection of the experiment.

## Who this guide is for

This guide is for biologists who:

- Have fluorescence microscopy images they need to quantify
- Can describe in words what they want to measure
- Are comfortable running commands in a terminal (or willing to learn)
- Have access to an AI coding assistant (Claude, ChatGPT, Copilot, etc.)
- Do not want to learn Python, R, or any other programming language to do their science

It is not for computational biologists who want a flexible image analysis framework. CellProfiler, napari, and scikit-image serve that audience well. It is not for deep learning researchers who want to train custom segmentation models. It is for the bench biologist who just took beautiful images and needs numbers.

## How to use this guide

The guide provides two complete tutorials:

1. **Mammalian stress granules** — a minimal example with two conditions, three channels, and replicate-level statistics. This teaches the basics: installation, directory structure, channel definitions, and interpreting output.

2. **Yeast temperature series** — a more complex example with six conditions, nucleolar segmentation, colocalization analysis, and spatial proximity measurements. This teaches the advanced features and shows the pipeline generalizing to a different organism.

Each tutorial is written as a step-by-step walkthrough with explicit commands to copy and paste. Screenshots show what correct output looks like at each stage. When parameters need adjustment, the guide explains *what to look at* (the QC overlays) and *what to ask the AI* (e.g., "The cells are being split in half — how do I fix this?").

The goal is that a biologist who has never used the command line can complete Tutorial 1 in under an hour and have quantitative results from their own images by the end of the day.
